{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53812eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dlib\n",
    "!pip install face_recognition\n",
    "!pip install opencv-python\n",
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b762f4-053b-445c-9dc2-263a1a68ae08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Encode the faces using OpenCV\n",
    "\n",
    "Before being able to recognise faces in images and videos, I first need to encode (or quantify) the faces in my training set. Keep in mind that I am not actually training a network here - the network (in the library 'face_recognition') has already been trained to create 128-d embeddings from a dataset of ~3 million images.\n",
    "\n",
    "I could alternatively train a network from scratch or even fine-tune the weights of an existing model, but that is too much to be done for many projects. Furthermore, I would need a lot of images to train the network from scratch. Instead, it is easier to use the pre-trained network and then use it to construct 128-d embeddings for each of the 30 faces in my dataset.\n",
    "\n",
    "During classification, I have used a simple KNN model and votes to conclude the final face classification. Other traditional machine learning models could be used here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebda177-9b2c-4c8d-a3a2-624f150b62bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "from imutils import paths\n",
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import dlib\n",
    "# import argparse\n",
    "# #construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument('-i', '--dataset', required=True,\n",
    "# \thelp='path to input directory of faces + images')\n",
    "# ap.add_argument('-e', '--encodings', required=True,\n",
    "# \thelp='path to serialized db of facial encodings')\n",
    "# ap.add_argument('-d', '--detection-method', type=str, default='cnn',\n",
    "# \thelp='face detection model to use: either `hog` or `cnn`')\n",
    "# args = vars(ap.parse_args())\n",
    "args = {}\n",
    "args['dataset'] = os.getcwd() + '\\\\dataset'               #path to input directory of faces and images\n",
    "args['encodings'] = os.getcwd() + '\\\\encodings.pickle'    #path to serialized db of facial encodings\n",
    "args['detection_method'] = 'cnn'                          #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8057d03-48f8-4d42-afae-9a50dd0478b8",
   "metadata": {},
   "source": [
    "# Create facial embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163e4376-1849-4c27-b1c4-496e2e165326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_facial_embeddings(args):\n",
    "    #grab the paths to the input images in our dataset\n",
    "    print('[INFO] quantifying faces...')\n",
    "    imagePaths = list(paths.list_images(args['dataset']))\n",
    "    #initialize the list of known encodings and known names\n",
    "    knownEncodings = []\n",
    "    knownNames = []\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        print(i, imagePath)\n",
    "\n",
    "    #OpenCV orders color channels in BGR, but the dlib actually expects RGB. The face_recognition module uses dlib, so we need to swap color spaces and name the new image rgb\n",
    "    ti = time.time()\n",
    "    print('[INFO] processing image...')\n",
    "    #loop over the image paths\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        #extract the person name from the image path\n",
    "        print('{}/{}'.format(i+1, len(imagePaths)), end=', ')\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "        #load the input image and convert it from BGR (OpenCV ordering) to dlib ordering (RGB)\n",
    "        image = cv2.imread(imagePath)\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #detect the (x,y)-coordinates of the bounding boxes corresponding to each face in the input image\n",
    "        boxes = face_recognition.face_locations(rgb,  model=args['detection_method'])\n",
    "        #compute the facial embedding for the face, ie, to turn the bounding boxes of the face into a list of 128 numbers\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "        #loop over the encodings\n",
    "        for encoding in encodings:\n",
    "            # add each encoding + name to our set of known names and encodings\n",
    "            knownEncodings.append(encoding)\n",
    "            knownNames.append(name)\n",
    "    print('Done!')\n",
    "    print('Time taken: {:.1f} minutes'.format((time.time() - ti)/60))\n",
    "\n",
    "    #dump the names and encodings to disk for future recall\n",
    "    #encodings.pickle contains the 128-d face embeddings for each face in our dataset\n",
    "    print('[INFO] serializing encodings...')\n",
    "    data = {'encodings': knownEncodings, 'names': knownNames}\n",
    "    f = open(args['encodings'], 'wb')\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f1c97-80f3-4c65-9d3a-2e3a55d4b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using CPU only, encoding 30 images required ~10min !!\n",
    "args = {}\n",
    "args['dataset'] = os.getcwd() + '\\\\dataset'               #path to input directory of faces and images\n",
    "args['encodings'] = os.getcwd() + '\\\\encodings.pickle'    #path to serialized db of facial encodings\n",
    "args['detection_method'] = 'cnn'                          #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "\n",
    "create_facial_embeddings(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b01cb-b0b3-4e92-8957-edaef3c9d1c1",
   "metadata": {},
   "source": [
    "# Recognise faces in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071a5448-ba20-4074-9632-a8e8140b9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "args = {}\n",
    "args['encodings'] = os.getcwd() + '\\\\encodings.pickle'        #path to serialized db of facial encodings\n",
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (1).jpg'    #path to input image\n",
    "args['detection_method'] = 'cnn'                              #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2759d964-fb77-4b87-a4d8-b18fde1ee415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognise_faces(args):\n",
    "    ti = time.time()\n",
    "    #load the known faces and embeddings\n",
    "    print('[INFO] loading encodings...')\n",
    "    data = pickle.loads(open(args['encodings'], 'rb').read())\n",
    "    #load the input image and convert it from BGR to RGB\n",
    "    image = cv2.imread(args['image'])\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #detect the (x,y)-coordinates of the bounding boxes corresponding to each face in the input image, then compute the facial embeddings for each face\n",
    "    print('[INFO] recognising faces...')\n",
    "    boxes = face_recognition.face_locations(rgb, model=args['detection_method'])\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    #initialize the list of names for each face detected\n",
    "    names = []\n",
    "\n",
    "    #loop over the facial embeddings\n",
    "    for encoding in encodings:\n",
    "        #attempt to match each face in the input image to our known encodings, function returns a list of True/False values, one for each known encoding\n",
    "        #Internally, the compare_faces function is computing the Euclidean distance between the candidate embedding and all faces in our known encodings\n",
    "        votes = face_recognition.compare_faces(data['encodings'], encoding)\n",
    "        #check to see if a match is found\n",
    "        if True in votes:\n",
    "            #find the corresponding names of all faces matched (vote==True)\n",
    "            matches = [name for name, vote in list(zip(data['names'], votes)) if vote == True]  \n",
    "            #determine the most frequently occuring name (note: in the unlikely event of a tie, Python will select first entry in the dictionary)\n",
    "            name = Counter(matches).most_common()[0][0]\n",
    "        else:\n",
    "            name = 'Unknown'\n",
    "        #update the list of names\n",
    "        names.append(name)\n",
    "\n",
    "    print([' '.join([e.title() for e in name.split('_')]) for name in names])\n",
    "    print('Time taken: {:.1f} seconds'.format(time.time() - ti))\n",
    "          \n",
    "    #visualise with bounding boxes and labeled names, loop over the recognised faces\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        #draw the predicted face name on the image\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(image, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    #Display the resulting frame, press 'q' to exit\n",
    "    window_text = args['image'].split(os.path.sep)[-1]\n",
    "    cv2.imshow(window_text, image)\n",
    "    while True:\n",
    "        #if the `q` key is pressed, break from the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "    #Save output image\n",
    "    cv2.imwrite(args['image'].rsplit('.', 1)[0] + '_output.jpg', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012914cf-68cb-4662-9919-358bb433781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (1).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d010fa-692e-4b90-957f-8ebaf2140729",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (2).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1db17e-5e93-4cd4-a7b0-330ccb731d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (3).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e165b7-940d-4dec-9110-50c25533d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (4).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8442c50-1a8f-4c6c-8ede-bbcd42b450cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (5).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d8846-fdfa-4b4c-91b1-a1b6ff56ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (6).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ba294-dacc-4351-a702-cd8eeac18d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f619512-3105-464b-9463-95a74a8dd3af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recognise faces in video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d9f0988-0d05-425c-a8b2-a319d5aeab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import face_recognition\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "args = {}\n",
    "args['encodings'] = os.getcwd() + '\\\\encodings.pickle'              #path to serialized db of facial encodings\n",
    "args['input'] = os.getcwd() + '\\\\video_test\\\\trailer.mp4'           #path to input video\n",
    "args['output'] = args['input'].rsplit('.', 1)[0] + '_output.avi'    #path to output video\n",
    "args['display'] = 1                                                 #display output frame to screen: yes or no\n",
    "args['detection_method'] = 'hog'                                    #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "#Choose 'hog' if using only CPU (no GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffdcbd36-bb8d-4443-8925-0af0446f0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognise_faces_video(args):\n",
    "    ti = time.time()\n",
    "    #load the known faces and embeddings\n",
    "    print('[INFO] loading encodings...')\n",
    "    data = pickle.loads(open(args['encodings'], 'rb').read())\n",
    "    #initialize pointer to vid file and vid writer\n",
    "    print('[INFO] processing video...')\n",
    "    stream = cv2.VideoCapture(args['input'])\n",
    "    writer = None    #optionally writing processed video frames to disk later, so initialize writer to None\n",
    "\n",
    "    #loop over frames from the video file stream\n",
    "    while True:\n",
    "        #grab next frame\n",
    "        (grabbed, frame) = stream.read()\n",
    "        #if frame was not grabbed, then we have reached the end of stream\n",
    "        if not grabbed:\n",
    "            break\n",
    "        #convert the input frame from BGR to RGB then resize it to have a width of 750px (to speedup processing)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rgb = imutils.resize(frame, width=750)\n",
    "        r = frame.shape[1] / float(rgb.shape[1])\n",
    "        #detect the (x,y)-coordinates of the bounding boxes corresponding to each face in the input frame, then compute the facial embeddings for each face\n",
    "        boxes = face_recognition.face_locations(rgb, model=args['detection_method'])\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "        names = []\n",
    "\n",
    "        #loop over the facial embeddings\n",
    "        for encoding in encodings:\n",
    "        #attempt to match each face in the input image to our known encodings, function returns a list of True/False values, one for each known encoding\n",
    "        #Internally, the compare_faces function is computing the Euclidean distance between the candidate embedding and all faces in our known encodings\n",
    "        votes = face_recognition.compare_faces(data['encodings'], encoding)\n",
    "        #check to see if a match is found\n",
    "        if True in votes:\n",
    "            #find the corresponding names of all faces matched (vote==True)\n",
    "            matches = [name for name, vote in list(zip(data['names'], votes)) if vote == True]  \n",
    "            #determine the most frequently occuring name (note: in the unlikely event of a tie, Python will select first entry in the dictionary)\n",
    "            name = Counter(matches).most_common()[0][0]\n",
    "        else:\n",
    "            name = 'Unknown'\n",
    "        #update the list of names\n",
    "        names.append(name)\n",
    "\n",
    "        #visualise with bounding boxes and labeled names, loop over the recognised faces\n",
    "        for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "            #rescale the face coordinates\n",
    "            top = int(top * r)\n",
    "            right = int(right * r)\n",
    "            bottom = int(bottom * r)\n",
    "            left = int(left * r)\n",
    "            #draw the predicted face name on the image\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "            y = top - 15 if top - 15 > 15 else top + 15\n",
    "            cv2.putText(frame, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "        #if the video writer is None *AND* output path is provided (to write the frame to disk)\n",
    "        if writer is None and args['output'] is not None:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'MJPG')    #to use the “MJPG” 4-character code\n",
    "            writer = cv2.VideoWriter(args['output'], fourcc, 24, (frame.shape[1], frame.shape[0]), True)    #output file path, fourcc, frames per second target, and frame dimensions\n",
    "        #if the writer is not None, write the frame with recognised faces to disk\n",
    "        if writer is not None:\n",
    "            writer.write(frame)\n",
    "\n",
    "        #check if displaying output frame to screen\n",
    "        if args['display'] == 1:\n",
    "            cv2.imshow('Video file', frame)\n",
    "            #if the `q` key is pressed, break from the loop\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    #do a bit of cleanup\n",
    "    cv2.destroyAllWindows()\n",
    "    stream.release()    #close video file pointers\n",
    "    #check if the video writer point needs to be released\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    print('Time taken: {:.1f} minutes'.format((time.time() - ti)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487c837-e055-49d7-b5e8-8047633151cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['input'] = os.getcwd() + '\\\\video_test\\\\trailer.mp4'\n",
    "args['output'] = args['input'].rsplit('.', 1)[0] + '_output.avi'\n",
    "recognise_faces_video(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ceca26-f62b-4295-96d4-4faf3b8e560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['input'] = os.getcwd() + '\\\\video_test\\\\lunch_scene.mp4'\n",
    "args['output'] = args['input'].rsplit('.', 1)[0] + '_output.avi'\n",
    "recognise_faces_video(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6309303-e1d1-48fb-92f1-14826aace9e6",
   "metadata": {},
   "source": [
    "The output videos are: \n",
    "\n",
    "trailer_output.avi https://youtu.be/BxfdMrhsEnw\n",
    "\n",
    "lunch_scene_output.avi https://youtu.be/MtBklF6ivmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06614fc-4bec-494c-9e38-f87431bb6428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7865f9d-1954-4069-9fd4-a3c1f22942d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recognise faces in webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168616d1-78e8-4520-945b-8dfa40c063e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import imutils\n",
    "from imutils import paths\n",
    "from imutils.video import VideoStream\n",
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160e3b4-e174-4674-afd3-a93dbfe4011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create facial embeddings\n",
    "args = {}\n",
    "args['dataset'] = os.getcwd() + '\\\\dataset_webcam'               #path to input directory of faces and images\n",
    "args['encodings'] = os.getcwd() + '\\\\encodings_webcam.pickle'    #path to serialized db of facial encodings\n",
    "args['detection_method'] = 'cnn'                                 #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "\n",
    "create_facial_embeddings(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a275d5-438f-4bfa-9dfc-4b2de5e1e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn on webcam\n",
    "args = {}\n",
    "args['encodings'] = os.getcwd() + '\\\\encodings_webcam.pickle'    #path to serialized db of facial encodings\n",
    "args['output'] = os.getcwd() + '\\\\webcam_test\\\\output.avi'       #path to output video\n",
    "args['display'] = 1                                              #display output frame to screen: yes or no\n",
    "args['detection_method'] = 'hog'                                 #face detection model to use: CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "#Choose 'hog' if using only CPU (no GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d780b84-d762-4799-939b-7980bb221a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = time.time()\n",
    "#load the known faces and embeddings\n",
    "print('[INFO] loading encodings...')\n",
    "data = pickle.loads(open(args['encodings'], 'rb').read())\n",
    "#initialize the video stream and pointer to output video file, then allow the camera sensor to warm up\n",
    "print('[INFO] starting video stream...')\n",
    "vs = VideoStream(src=0).start()    #use VideoStream to access webcam, use src=1 for second webcam\n",
    "time.sleep(2.0)    #time.sleep with 2 seconds to warm up webcam\n",
    "writer = None    #optionally writing processed video frames to disk later, so initialize writer to None\n",
    "\n",
    "#loop over frames from the video file stream\n",
    "while True:\n",
    "    #grab a frame from the threaded video stream\n",
    "    frame = vs.read()\n",
    "    #convert the input frame from BGR to RGB then resize it to have a width of 750px (to speedup processing)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    rgb = imutils.resize(frame, width=750)\n",
    "    r = frame.shape[1] / float(rgb.shape[1])\n",
    "    #detect the (x,y)-coordinates of the bounding boxes corresponding to each face in the input frame, then compute the facial embeddings for each face\n",
    "    boxes = face_recognition.face_locations(rgb, model=args['detection_method'])\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    names = []\n",
    "    #loop over the facial embeddings\n",
    "    for encoding in encodings:\n",
    "        #attempt to match each face in the input image to our known encodings, function returns a list of True/False values, one for each known encoding\n",
    "        #Internally, the compare_faces function is computing the Euclidean distance between the candidate embedding and all faces in our known encodings\n",
    "        votes = face_recognition.compare_faces(data['encodings'], encoding)\n",
    "        #check to see if a match is found\n",
    "        if True in votes:\n",
    "            #find the corresponding names of all faces matched (vote==True)\n",
    "            matches = [name for name, vote in list(zip(data['names'], votes)) if vote == True]  \n",
    "            #determine the most frequently occuring name (note: in the unlikely event of a tie, Python will select first entry in the dictionary)\n",
    "            name = Counter(matches).most_common()[0][0]\n",
    "        else:\n",
    "            name = 'Unknown'\n",
    "        #update the list of names\n",
    "        names.append(name)\n",
    "\n",
    "    #visualise with bounding boxes and labeled names, loop over the recognised faces\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        #rescale the face coordinates\n",
    "        top = int(top * r)\n",
    "        right = int(right * r)\n",
    "        bottom = int(bottom * r)\n",
    "        left = int(left * r)\n",
    "        #draw the predicted face name on the image\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(frame, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    #if the video writer is None *AND* output path is provided (to write the frame to disk)\n",
    "    if writer is None and args['output'] is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')    #to use the “MJPG” 4-character code\n",
    "        writer = cv2.VideoWriter(args['output'], fourcc, 20, (frame.shape[1], frame.shape[0]), True)    #output file path, fourcc, frames per second target, and frame dimensions\n",
    "    #if the writer is not None, write the frame with recognised faces to disk\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "        \n",
    "    #check if displaying output frame to screen\n",
    "    if args['display'] == 1:\n",
    "        cv2.imshow('Webcam', frame)\n",
    "        #if the `q` key is pressed, break from the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "#do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n",
    "#check if the video writer point needs to be released\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "print('Done! \\nTime taken: {:.1f} minutes'.format((time.time() - ti)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caeb733-f05c-4088-8f1e-a5bc6b1ab42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776518d5-181c-4267-95c5-06c43b2c216e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-cv2",
   "language": "python",
   "name": "env-cv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
